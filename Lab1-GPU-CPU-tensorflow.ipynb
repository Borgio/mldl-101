{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using GPU's on IBM POWER S822LC \n",
    "![](https://github.com/dustinvanstee/random-public-files/raw/master/s822lc_nvidia.png)\n",
    "\n",
    "In this lab, you will familiarize yourself with how to compare GPU vs CPU performance using some simple matrix math.  We will cover how tensorflow gives access to GPU and CPU, and how you can specify to the system when they can be used.\n",
    "\n",
    "You don't need to know tensorflow to complete this lab, but understanding some of tensorflow basics and matrix math will help."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lets get started, first lets import the tensorflow library that has been provided by the IBM PowerAI library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import math\n",
    "# Time helper\n",
    "import timeit\n",
    "\n",
    "# Print Python Version\n",
    "import os\n",
    "import sys\n",
    "print(sys.version)\n",
    "import gpulab_funcs \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Verify GPU Usage with Small Matrix\n",
    "Now lets build a few small matrixes and multiply them together.  Here A is 2x3 matrix, an B is 3x2 matrix.  Then we multiply them together.  By default tensorflow will use the GPU on the system if available.  Lets run this code and verify that we are using the GPU\n",
    "\n",
    "Note : To find out which devices your operations and tensors are assigned to, create the session with log_device_placement configuration option set to True as shown below.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Creates a graph.\n",
    "tf.reset_default_graph() \n",
    "with tf.device('/gpu:0'):\n",
    "  a = tf.constant([1.0, 2.0, 3.0, 4.0, 5.0, 6.0], shape=[2, 3], name='a')\n",
    "  b = tf.constant([1.0, 2.0, 3.0, 4.0, 5.0, 6.0], shape=[3, 2], name='b')\n",
    "  c = tf.matmul(a, b)\n",
    "# Creates a session with log_device_placement set to True.\n",
    "with tf.Session() as sess :\n",
    "    writer = tf.summary.FileWriter('./graphs', graph=tf.get_default_graph())\n",
    "    sess = tf.Session(config=tf.ConfigProto(log_device_placement=True))\n",
    "    # Runs the op.\n",
    "    print(sess.run(c))\n",
    "    sess.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we started the jupyter notebook, we also redirected all the messages to a log file so that we can see if indeed we are using the GPU.  Lets read the last couple of lines in the log file (/tmp/tensorflow.log)\n",
    "\n",
    "Look for `gpu:0` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "tail -n 10 /tmp/tensorflow.log | grep gpu\n",
    "#echo \" ############\" >>  /tmp/tensorflow.log "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here you can see that 3 operations were assigned to gpu:0.  Two constant assignments a,b, and a matrix multiply (MatMul)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Verify that we can use the CPU as well with same example\n",
    "\n",
    "Here we will specify explicitly that we want to use the CPU.  By convention, tensorflow allows you to specify that by using the tf.device() method.\n",
    "\n",
    "Devices are represented by the following strings\n",
    "\n",
    "* /cpu:0 : The first cpu of your machine\n",
    "* /gpu:0 : The first gpu of your machine\n",
    "* /gpu:1 : The second gpu of your machine\n",
    "* ...\n",
    "* ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this cell we specify **/cpu:0** with the exact same code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "tf.reset_default_graph() \n",
    "# Creates a graph.\n",
    "with tf.device('/cpu:0'):\n",
    "  a = tf.constant([1.0, 2.0, 3.0, 4.0, 5.0, 6.0], shape=[2, 3], name='a')\n",
    "  b = tf.constant([1.0, 2.0, 3.0, 4.0, 5.0, 6.0], shape=[3, 2], name='b')\n",
    "  c = tf.matmul(a, b)\n",
    "\n",
    "# Creates a session with log_device_placement set to True.\n",
    "with tf.Session() as sess :\n",
    "    writer = tf.summary.FileWriter('./graphs', graph=tf.get_default_graph())\n",
    "    sess = tf.Session(config=tf.ConfigProto(log_device_placement=True))\n",
    "    # Runs the op.\n",
    "    print(sess.run(c))\n",
    "    sess.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets make sure that we used the cpu ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "tail -n 10 /tmp/tensorflow.log | grep cpu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice here that now we are mapping all the operations to the cpu. \n",
    "\n",
    "\n",
    "Also notice we dont really get a large speedup.  Thats because the example is to small to make use of the parallelism advantage inherent in the GPU.  Next, we will rerun but this time will very large matrices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GPU Speedup test with 20 Billion cells\n",
    "\n",
    "Here we change the matrices to be sets of randomly distributed values, and multiply them together.  The resultant matrix from the mat1 * mat2 operation will be a 20000 x 10000 matrix.  Run this cell and notice the run time to be approx ~1 s (run this cell a couple times to get a feel for the average)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "tf.reset_default_graph() \n",
    "\n",
    "mat1 = tf.random_normal([20000,10000],mean=0.0, stddev=1.0, dtype=tf.float32)\n",
    "mat2 = tf.random_normal([10000,10000],mean=0.0, stddev=1.0, dtype=tf.float32)\n",
    "mat3 = tf.matmul(mat1, mat2)\n",
    "\n",
    "# Creates a session with log_device_placement set to True.\n",
    "with tf.Session() as sess :\n",
    "    writer = tf.summary.FileWriter('./graphs', graph=tf.get_default_graph())\n",
    "    sess = tf.Session(config=tf.ConfigProto(log_device_placement=True))\n",
    "    # Runs the op.\n",
    "    sess.run(mat3)\n",
    "    sess.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### CPU test with 20 Billion cells\n",
    "Now lets perform the same operation with CPU only.  \n",
    "\n",
    "If you were to check vmstat on your terminal, you would notice that all 32 CPU cores are working to multiply this matrix.   You can verify this by running <p>\n",
    "**`vmstat 3`** <p> at the terminal in your other browser window you had setup.\n",
    "\n",
    "Sample output will show 32 processes running in parallel on the cpus <p>\n",
    "`32  0 1433856 129245184      0     80    0    0     0     0 11183 5409 100  0  0  0  0`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "tf.reset_default_graph() \n",
    "\n",
    "with tf.device('/cpu:0'):\n",
    "    mat1 = tf.random_normal([20000,10000],mean=0.0, stddev=1.0, dtype=tf.float32)\n",
    "    mat2 = tf.random_normal([10000,10000],mean=0.0, stddev=1.0, dtype=tf.float32)\n",
    "    mat3 = tf.matmul(mat1, mat2)\n",
    "\n",
    "    # Creates a session with log_device_placement set to True.\n",
    "with tf.Session() as sess :\n",
    "    writer = tf.summary.FileWriter('./graphs', graph=tf.get_default_graph())\n",
    "    sess = tf.Session(config=tf.ConfigProto(log_device_placement=True))\n",
    "    # Runs the op.\n",
    "    sess.run(mat3)\n",
    "    sess.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*** Notice we achieved between a 30 to 50x speedup with the GPU for this testcase! ***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lets find the knee of the curve\n",
    "\n",
    "For this exersize, manually populate different values within the runs list, and then plot the results.  Determine the knee of the curve.  The knee is the location where the size of the matrix can start to exploit the parallelism offered by the GPU.  Try to run the cell below to get a feel for multiply square matrices of 10,100.\n",
    "\n",
    "Max run size should be less than 25000 to finish within a reasonable time\n",
    "\n",
    "\n",
    " <div class=\"panel-group\" id=\"accordion-11\">\n",
    "  <div class=\"panel panel-default\">\n",
    "    <div class=\"panel-heading\">\n",
    "      <h4 class=\"panel-title\">\n",
    "        <a data-toggle=\"collapse\" data-parent=\"#accordion-11\" href=\"#collapse1-11\">\n",
    "        Answer</a>\n",
    "      </h4>\n",
    "    </div>\n",
    "    <div id=\"collapse1-11\" class=\"panel-collapse collapse\">\n",
    "      <div class=\"panel-body\">modify runs = [10,100,300,700,1000,3000,7000,10000,13000,17000] </div>\n",
    "    </div>\n",
    "  </div>\n",
    "</div> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "### Edit code below this line\n",
    "matrix_sizes = [10,100,1000,10000]  # this is the \n",
    "### Dont edit code below this line\n",
    "\n",
    "\n",
    "# ratios will be populated via the getRatio function\n",
    "ratios = []\n",
    "for i in matrix_sizes :\n",
    "    ratios.append(gpulab_funcs.getRatio(i))\n",
    "\n",
    "print matrix_sizes\n",
    "print ratios"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot your results here\n",
    "Your array sizes were save in the runs list, and ratios were saved in the ratios list.  Lets plot the results to determine where the knee of the curve is.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot\n",
    "%matplotlib inline\n",
    "\n",
    "matplotlib.pyplot.scatter(matrix_sizes,ratios)\n",
    "matplotlib.pyplot.xscale('log')\n",
    "matplotlib.pyplot.xlabel('Array Dimension Size', fontsize=18)\n",
    "matplotlib.pyplot.ylabel('GPU Speedup', fontsize=18)\n",
    "matplotlib.pyplot.show()\n",
    "\n",
    "# note , run this cell twice.  for some reason the first time does not plot the graph ...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary\n",
    "\n",
    "In this lab you were able to see how to use tensorflow and see how a GPU can speedup matrix multiplication operations.  In general, matrix operations are used frequently in machine learning algorithms, and hence the GPU capability will speed a number of ML algorithms.  In the next labs we will be exclusively using the GPU for image classification and churn examples. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "useful stackoverflows that contributed to this lab<p>\n",
    "https://stackoverflow.com/questions/46178961/duplicate-tensorflow-placeholder-variables<p>\n",
    "https://stackoverflow.com/questions/37660312/run-tensorflow-on-cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
